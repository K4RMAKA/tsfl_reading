{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未完成整理：\n",
    "* 数据采集／获取\n",
    "* Tensorflow内置的Machine Learning部分函数\n",
    "* 学习资源汇总"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一章的学习要点\n",
    "\n",
    " * 用来将数据正则化：data = tf.nn.batch_norm_with_global_normalization(...)\n",
    "\n",
    "【问题】：正则化就是我所理解的那个正则化吗？（减去期待值后再除以方差）\n",
    " \n",
    " * 标准建模的流程：\n",
    " \n",
    " import/generate datasets ---> Transform and normalize data ---> Partition datasets into train,test and validation sets ---> set algorithm parameters(hyperparameters) ---> initialize variables and placeholders --->define model structure ---> declare the loss function ---> initialize and train the model ---> evaluate the model ---> tune hyperparameters ---> deploy/predict new outcomes\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成一般Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成一个全是0的Tensor\n",
    "zero_tensor = tf.zeros([3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成一个全是1的Tensor\n",
    "ones_tensor = tf.ones([3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Fill_1:0\", shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#生成一个全是特定常数的Tensor\n",
    "filled_tensor = tf.fill([3,3],30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#根据一个已经存在的常量生成Tensor\n",
    "constant_tensor = tf.constant([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones_like_4:0\", shape=(2, 3), dtype=int32)\n",
      "Tensor(\"Fill_3:0\", shape=(2, 3), dtype=int32)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#根据一个已有的Tensor的结构来生成一个Tensor\n",
    "zeros_similar = tf.zeros_like(constant_tensor)\n",
    "ones_similar = tf.ones_like(constant_tensor)\n",
    "ones_compare = tf.fill([2,3],1)\n",
    "print(ones_similar)\n",
    "print(ones_compare)\n",
    "#从下面那个比较可知比较Tensor时应该也是类似于比较对象，仅仅是所有属性相同并不等于相等\n",
    "#应该时对象间的比较\n",
    "print(ones_similar == ones_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成序列Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_tensor = tf.linspace(0.0,1.0,5)\n",
    "#上述函数的包含了终止数值，即右包含\n",
    "linear_seq_tensor = tf.range(6,15,3)\n",
    "#range的话就不包含右边的数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成随机Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成uniform distribution\n",
    "#生成的随机数值不包含maxval\n",
    "randunif_tensor = tf.random_uniform([3,3],minval=0,maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成normal distribution\n",
    "randnorm_tensor = tf.random_normal([3,3],mean=0,stddev=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#生成受限的normal distribution\n",
    "#生成的是两个标准差之内的数值\n",
    "runcnorm_tensor = tf.truncated_normal([3,3],mean=0, stddev=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【问题】：suhffle到底shuffle的是什么东西来着……有点忘了\n",
    "【问题】：crop也是谜一般的玩意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random shuffle\n",
    "shuffled_output = tf.random_shuffle(runcnorm_tensor)\n",
    "#crop Tensor\n",
    "\n",
    "#random_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_var = tf.Variable(tf.zeros([3,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将各种形式的变量转换为Tensor的万能函数（？）\n",
    "【问题】：为什么能在convert_to_tensor的参数里面使用Tensor？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_7:0\", shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "new_var = tf.convert_to_tensor([1,2,3])\n",
    "print(new_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 变量的初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#所有的Variable在使用前都必须进行初始化，每个Variable自身都有initializer，但是每个\n",
    "#Variable都单独进行初始化有时候比较麻烦所以用global_variable_initializer也比较\n",
    "#常见\n",
    "my_var = tf.Variable(tf.zeros(2,3))\n",
    "sess = tf.Session()\n",
    "initialize_op = tf.global_variables_initializer()\n",
    "sess.run(initialize_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#但是有的时候如果其中一个Tensor的结构依赖于另外一个Tensor，就必须进行单独的初始化\n",
    "sess = tf.Session()\n",
    "first_val = tf.Variable(tf.zeros([2,3]))\n",
    "sess.run(first_val.initializer)\n",
    "second_val = tf.Variable(tf.zeros_like(first_val))\n",
    "sess.run(second_val.initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Placeholder使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.4646773   0.75074887]\n",
      " [ 0.04458335  0.01987305]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "x = tf.placeholder(tf.float32,shape=[2,2])\n",
    "y = tf.identity(x)\n",
    "x_vals = np.random.rand(2,2)\n",
    "print(sess.run(x,feed_dict={x:x_vals}))\n",
    "#如果上面写成sess.run(x,feed_dict={x:x_vals})也不会出循环引用的问题？可能Tensorflow\n",
    "#后面几个版本有进行优化吧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成matrices&matrix各种特性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identity_matrix = tf.diag([1.0,1.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = tf.truncated_normal([2,3])\n",
    "B = tf.random_uniform([3,2])\n",
    "C = tf.fill([3,3],5.0)\n",
    "D = tf.convert_to_tensor([[1.,2.,3.],[-3.,-7.,-1.],[0.,5.,-2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(identity_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.  5.  5.]\n",
      " [ 5.  5.  5.]\n",
      " [ 5.  5.  5.]]\n"
     ]
    }
   ],
   "source": [
    "#矩阵乘法\n",
    "print(sess.run(tf.matmul(identity_matrix,C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.21678245  0.19870323  1.13959551]\n",
      " [-0.04773329 -0.52218384 -0.05092119]]\n",
      "[[ 0.01096273  0.00436912 -0.01823799]\n",
      " [ 0.00436912  1.18608701 -1.56131327]\n",
      " [-0.01823799 -1.56131327  2.06948829]]\n"
     ]
    }
   ],
   "source": [
    "#矩阵转置\n",
    "#每次sess.run都会重新生成一次Tensor\n",
    "print(sess.run(A))\n",
    "print(sess.run(tf.matmul(tf.transpose(A),A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.50000006 -0.5        -0.50000006]\n",
      " [ 0.15789475  0.05263158  0.21052633]\n",
      " [ 0.39473686  0.13157895  0.0263158 ]]\n",
      "----------------\n",
      "[[  9.99999940e-01  -2.38418579e-07  -1.19209290e-07]\n",
      " [  0.00000000e+00   1.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   2.98023224e-08   1.00000000e+00]]\n",
      "-38.0\n"
     ]
    }
   ],
   "source": [
    "#determinant and inverse\n",
    "#Tensorflow的inverse在matrix正定的时候是基于Cholesky分解，其他时候是基于LU分界的\n",
    "print(sess.run(tf.matrix_inverse(D)))\n",
    "print(\"----------------\")\n",
    "print(sess.run(tf.matmul(tf.matrix_inverse(D),D)))\n",
    "print(sess.run(tf.matrix_determinant(D)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【问题】：把Cholesky分解和LU分解再看一遍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#分解\n",
    "print(sess.run(tf.cholesky(identity_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-10.65907574,  -0.22750695,   2.88658309], dtype=float32), array([[-0.21749546, -0.63250113, -0.74339646],\n",
      "       [-0.84526515, -0.25879988,  0.46749276],\n",
      "       [ 0.48808062, -0.7300446 ,  0.47834331]], dtype=float32))\n",
      "[-10.65907574  -0.22750695   2.88658309]\n",
      "[[-0.21749546 -0.63250113 -0.74339646]\n",
      " [-0.84526515 -0.25879988  0.46749276]\n",
      " [ 0.48808062 -0.7300446   0.47834331]]\n"
     ]
    }
   ],
   "source": [
    "#eigen特征值和特征向量\n",
    "print(sess.run(tf.self_adjoint_eig(D)))\n",
    "#这样得到的结果是一个tuple，其中第一项是特征值，第二项是特征向量的list，可以通过tuple分\n",
    "#解取出，这种tuple的结构叫做Eigen decomposition of a matrix\n",
    "eigen_value, eigen_vector = tf.self_adjoint_eig(D)\n",
    "print(sess.run(eigen_value))\n",
    "print(sess.run(eigen_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor基本运算符（其实是函数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.75\n",
      "-------------------\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "#div默认返回参数类型，所以参数类型为Int的时候返回值也会为Int，所以要养成加.的好习惯\n",
    "#类似于Python3的除法要用truediv\n",
    "print(sess.run(tf.div(3,4)))\n",
    "print(sess.run(tf.div(3.,4)))\n",
    "print(\"-------------------\")\n",
    "print(sess.run(tf.truediv(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#floor\n",
    "#返回值依然依据参数类型，但是会向下取整到最大的整数\n",
    "print(sess.run(tf.floordiv(3.,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#mod\n",
    "#返回值依据参数类型\n",
    "print(sess.run(tf.mod(22.,5.)))\n",
    "print(sess.run(tf.mod(22,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其余常用的函数：\n",
    " * abs()\n",
    " * ceil()\n",
    " * cos()\n",
    " * exp()\n",
    " * floor()\n",
    " * inv()\n",
    " * log()\n",
    " * maximum()\n",
    " * minimum()\n",
    " * neg()\n",
    " * pow()\n",
    " * round()\n",
    " * rsqrt()\n",
    " * sign()\n",
    " * sin()\n",
    " * sqrt()\n",
    " * square()\n",
    " * cross()\n",
    " \n",
    " 注意所有的函数都是element-wise的，以maximum为例的话，就是返回的Tensor中的每一个\n",
    " dimension上的值都是该dimension上的最大值\n",
    " \n",
    " 【问题】：cross到底是什么玩意……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用于Machine Learning的一些内置函数：\n",
    " * digamma\n",
    " * erf\n",
    " * erfc\n",
    " * igamma\n",
    " * igammac\n",
    " * lbeta\n",
    " * lgamma\n",
    " * squared_difference\n",
    " \n",
    " 这个感觉好复杂了……之后又时间再整理吧\n",
    " \n",
    " 【问题】：把上述函数的作用和原理刷一遍……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 虽然Tensorflow里内置了很多函数，但是更多情况下都是要基于这些基本函数来生成自己真正需要的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "#生成一个polynomial函数\n",
    "def custom_polynomial(value):\n",
    "    return tf.subtract(3 * tf.square(value), value) + 10\n",
    "print(sess.run(custom_polynomial(5)))\n",
    "#一个重要的细节：老版本的Tensorflow中的减法是sub，但是新版本换成了substract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "激活函数是内置于nn库的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  3 10]\n"
     ]
    }
   ],
   "source": [
    "#最常见的激活函数ReLU\n",
    "print(sess.run(tf.nn.relu([-3,3,10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 6]\n"
     ]
    }
   ],
   "source": [
    "#也可以给ReLU函数的正方向加个Cap，函数表示的话就是min(max(0,x),Cap),这在nn库中的\n",
    "#实现就是relu6\n",
    "print(sess.run(tf.nn.relu6([-3,3,10])))\n",
    "#relu6是一种hard-sigmoid函数，好处是计算更快而且不容易出现vanishing或exploding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【问题】：仔细思考为什么relu6有上述的那些好处，搞清楚hard-sigmoid到底是什么定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26894143  0.5         0.7310586 ]\n"
     ]
    }
   ],
   "source": [
    "#最常用／老出问题的激活函数sigmoid function/logistic function\n",
    "#函数表示：1/(1+exp(-x))\n",
    "#返回值永远在0和1之间\n",
    "#老是出现vanishing\n",
    "print(sess.run(tf.nn.sigmoid([-1.,0.,1.])))\n",
    "#重要tips：sigmoid使用的时候参数只能是float或者double，传入int的话会出错！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.76159418  0.          0.76159418]\n"
     ]
    }
   ],
   "source": [
    "#不那么长用／姑且记一下的激活函数hyperbolic tangent\n",
    "#函数表示：(exp(x) - exp(-x))/(exp(x) + exp(-x))\n",
    "print(sess.run(tf.nn.tanh([-1.,0,1])))\n",
    "#与sigmoid函数相同，参数只能是float或者double，但是list的时候只要其中任何一个元素的\n",
    "#类型是float／double，整个list都会强制升级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  0.   0.5]\n"
     ]
    }
   ],
   "source": [
    "#第一次见的激活函数softsign\n",
    "#如名字所提示，是sign的连续近似型\n",
    "#函数表示：x/abs(x)+1\n",
    "print(sess.run(tf.nn.softsign([-1.,0,1])))\n",
    "#允许传入int，但是传入后返回值也变成int了……算了以后还是老老实实所有list都变符点吧……"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.31326166  0.69314718  1.31326163]\n"
     ]
    }
   ],
   "source": [
    "#同～第一次见的激活函数softplus\n",
    "#函数表示：log(exp(x)+1)\n",
    "print(sess.run(tf.nn.softplus([-1.,0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.63212055  0.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "#意外的经常见到的激活函数Exponential Linear Unit(ELU)\n",
    "#感觉算是ReLU的改良型之一？\n",
    "#函数表示：(exp(x)+1) if x<0 else x\n",
    "print(sess.run(tf.nn.elu([-1.,0,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重要！！activation function的选择\n",
    "\n",
    "activation function的选择要依据所传入参数的类型，一般情况下我们倾向于保留参数／数据的统计学特征，所以当传入的数据期待值为0时，为了保持住variance就很明显不能使用sigmoid（sigmoid的返回值是0到1，会大量改变数据特征），更好的选择是tanh或者softsign\n",
    "\n",
    "ざっくり分類してみよう！\n",
    "\n",
    "* ReLU系：ReLU,ReLU6,Softplus,ELU\n",
    "* Sigmoid系：Sigmoid,Tanh,Softsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
